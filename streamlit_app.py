import streamlit as st
import pandas as pd
from openai import OpenAI
import tiktoken

# í† í° ìˆ˜ ê³„ì‚° í•¨ìˆ˜


def count_tokens(text, model="gpt-3.5-turbo"):
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ê³„ì‚°í•©ë‹ˆë‹¤."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# ëª…ë ¹ì–´ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜


def generate_prompt(use_case, document, question):
    """
    Create a prompt based on the selected use case. 
    Focus on providing deep insights, numerical evidence (if relevant), 
    and clear, actionable recommendations.
    """

    base_instructions = (
        "You are a professional AI assistant specializing in data-driven marketing insights. "
        "Analyze the provided data from multiple perspectives, offering deeper insights "
        "and evidence that basic charts or tables might not reveal."
    )

    if use_case == "ë°ì´í„° ìš”ì•½ ë° ë¶„ì„":
        prompt = (
            f"{base_instructions}\n\n"
            "Use Case: Data Summary and Analysis\n"
            "Task: Provide a comprehensive analysis of the given data, including trends, patterns, and anomalies.\n"
            "Requirements:\n"
            "1. Explain the meaning and relevance of each field.\n"
            "2. Where possible, Cross-examine multiple metrics to uncover insights beyond simple charts or tables.\n"
            f"Data:\n{document}\n\n"
            f"Question:\n{question}\n"
        )

    elif use_case == "ë§ˆì¼€íŒ… ì „ëµ ì œì•ˆ":
        prompt = (
            f"{base_instructions}\n\n"
            "Use Case: Marketing Strategy Recommendations\n"
            "Task: Propose effective marketing strategies (channels, targeting, budget, etc.) "
            "based on the data.\n"
            "Requirements:\n"
            "1. Leverage data-driven insights to form detailed action plans.\n"
            "2. Provide numerical or factual evidence to support each recommendation.\n"
            "3. If possible, outline expected KPIs or success metrics.\n\n"
            f"Data:\n{document}\n\n"
            f"Question:\n{question}\n"
        )

    elif use_case == "ë°ì´í„° ë¹„êµ ë° í‰ê°€":
        prompt = (
            f"{base_instructions}\n\n"
            "Use Case: Data Comparison and Evaluation\n"
            "Task: Compare the given datasets, identify key differences and similarities, "
            "and propose improvements.\n"
            "Requirements:\n"
            "1. Compare key metrics or trends across the datasets.\n"
            "2. Identify correlations or disparities and highlight potential causes.\n"
            "3. Suggest follow-up analyses or data to gather, if needed.\n\n"
            f"Data:\n{document}\n\n"
            f"Question:\n{question}\n"
        )

    else:
        # Default: Data Summary and Analysis
        prompt = (
            f"{base_instructions}\n\n"
            "Use Case: Data Summary and Analysis\n"
            "Task: Provide a comprehensive analysis of the given data, including trends, patterns, and anomalies.\n"
            "Requirements:\n"
            "1. Explain the meaning and relevance of each field.\n"
            "2. Cross-examine multiple metrics to uncover insights beyond simple charts or tables.\n"
            "3. Where possible, suggest data visualizations or additional interpretation tips.\n\n"
            f"Data:\n{document}\n\n"
            f"Question:\n{question}\n"
        )

    return prompt


# ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€
st.title("ğŸ“Š ë§ˆì¼€íŒ… ë°ì´í„° ë¶„ì„ ë„ìš°ë¯¸")
st.write(
    """
    ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•œ í›„, ë¶„ì„ ëª©ì (ìœ ìŠ¤ì¼€ì´ìŠ¤)ì„ ì„ íƒí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.
    ì„ íƒí•œ ìœ ìŠ¤ì¼€ì´ìŠ¤ì— ë”°ë¼ GPTê°€ ë°ì´í„°ë¥¼ í•´ì„í•˜ê³  ì ì ˆí•œ ë§ˆì¼€íŒ… ì•¡ì…˜ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì—¬ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
)

# OpenAI API í‚¤ ì…ë ¥
openai_api_key = st.text_input("OpenAI API í‚¤ ì…ë ¥", type="password")

if not openai_api_key:
    st.info("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.", icon="ğŸ—ï¸")
else:
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    client = OpenAI(api_key=openai_api_key)

    # ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ
    st.write("### ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ")
    input_method = st.radio(
        "ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì…ë ¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        ("CSV íŒŒì¼ ì—…ë¡œë“œ", "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥"),
    )

    # ë°ì´í„° ì—…ë¡œë“œ ë˜ëŠ” ì…ë ¥
    document = ""
    if input_method == "CSV íŒŒì¼ ì—…ë¡œë“œ":
        uploaded_file = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=("csv",))
        if uploaded_file:
            try:
                df = pd.read_csv(uploaded_file)
                document = df.to_csv(index=False)
                st.write("ì—…ë¡œë“œí•œ CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
                st.dataframe(df)
            except Exception as e:
                st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    elif input_method == "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥":
        document = st.text_area(
            "í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ë¶„ì„í•  ë°ì´í„°ë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.",
        )

    # ìœ ìŠ¤ì¼€ì´ìŠ¤ ì„ íƒ
    st.write("### ë¶„ì„ ëª©ì (ìœ ìŠ¤ì¼€ì´ìŠ¤) ì„ íƒ")
    use_case = st.selectbox(
        "ì›í•˜ëŠ” ë¶„ì„ ëª©ì ì„ ì„ íƒí•˜ì„¸ìš”:",
        [
            "ë°ì´í„° ìš”ì•½ ë° ë¶„ì„",
            "ë§ˆì¼€íŒ… ì „ëµ ì œì•ˆ",
            "ë°ì´í„° ë¹„êµ ë° í‰ê°€",
        ],
    )

    # ëª¨ë¸ ì„ íƒ
    st.write("### ëª¨ë¸ ì„ íƒ")
    model = st.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
        [
            "o1-mini-2024-09-12",
            "o1-preview-2024-09-12",
            "gpt-4",
            "gpt-3.5-turbo",
        ],
        index=0  # ê¸°ë³¸ ì„ íƒê°’
    )

    # ì§ˆë¬¸ ì…ë ¥
    st.write("### ì§ˆë¬¸ ì…ë ¥")
    max_tokens = 5000  # ìµœëŒ€ í† í° ìˆ˜
    question = st.text_area(
        "ë°ì´í„°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ì´ ë°ì´í„°ì—ì„œ ì£¼ìš” íŠ¸ë Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        disabled=not document.strip(),  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹„í™œì„±í™”
    )

    # ì…ë ¥ í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ
    total_tokens = count_tokens(
        document, model=model) + count_tokens(question, model=model)
    st.caption(f"ì‚¬ìš©í•œ ì…ë ¥ í† í° ìˆ˜: **{total_tokens}**/{max_tokens}")

    if total_tokens > max_tokens:
        st.error("ì…ë ¥ í† í° ìˆ˜ê°€ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ ì¤„ì´ì„¸ìš”.")

    # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
    if document.strip() and question.strip() and total_tokens <= max_tokens:
        try:
            # ëª…ë ¹ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = generate_prompt(use_case, document, question)

            # ë¡œë”© ìƒíƒœ í‘œì‹œ
            with st.spinner("GPTê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                stream = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    stream=True
                )

            # ë‹µë³€ í‘œì‹œ
            st.write_stream(stream)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
